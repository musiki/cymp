---
type: llm
img: https://i.imgur.com/rCWWE8a.png
name: n-gram models
year: 1990
tags:
  - statistical
  - shallow
  - probabilistic
tech_innovation: word-level Markov modeling of language sequences
param_million: n^3
authors:
  - "[[Claude Shannon]]"
  - "[[Peter Brown]]"
  - "[[Stephen Della Pietra]]"
---



Un modelo de n-gramas guarda probabilidades para cada secuencia de palabras de longitud n.
En lugar de ‚Äúpar√°metros aprendidos‚Äù como en redes neuronales, tiene un conteo de frecuencias que se normaliza a probabilidades.

Ejemplo sencillo:
- Si tu corpus tiene 100.000 palabras en el vocabulario (V),
- un modelo trigrama (n=3) tendr√≠a un espacio potencial de combinaciones de:

$V^3 = 100,000^3 = 10^{15}$


Pero en la pr√°ctica, solo una peque√±a fracci√≥n aparece en el texto, as√≠ que se almacenan muchos menos.

üëâ Si estimamos que en un corpus grande de la √©poca (por ejemplo, el British National Corpus o Google N-grams) podr√≠as tener cientos de millones de trigramas distintos, entonces un modelo n-grama a gran escala pod√≠a ocupar del orden de 100M‚Äì1B par√°metros equivalentes (dependiendo del vocabulario y corpus).
Esto es comparable al tama√±o de GPT-2 Small (124M) o incluso al GPT-2 Medium (355M) en espacio de memoria, ¬°pero mucho menos eficiente en generalizaci√≥n!


‚∏ª

## Investigadores y or√≠genes de los n-gramas

- Claude Shannon (1948) fue el primero en describir el uso de cadenas de Markov (n-gramas) para modelar lenguaje en A Mathematical Theory of Communication.
- En ling√º√≠stica computacional, los n-gramas se popularizaron en los a√±os 80‚Äì90 como la base de los sistemas de reconocimiento de voz y traducci√≥n autom√°tica.
- Peter Brown, Stephen Della Pietra, Vincent Della Pietra y Robert Mercer (IBM, 1990s) usaron modelos n-grama en el famoso proyecto de traducci√≥n autom√°tica estad√≠stica.
- M√°s tarde, Joshua Goodman y Stanley Chen (Harvard/Microsoft Research) refinaron la estimaci√≥n y el smoothing (Good-Turing, Kneser-Ney).
- Google public√≥ su Google N-gram Corpus (2006), que conten√≠a billones de palabras y trillones de n-gramas, empujando su uso masivo en aplicaciones de la √©poca.